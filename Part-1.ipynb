{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine learning vs. deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning and deep learning can both be described as artificial intelligence. In both cases, a computer system learns from data to make intelligent decisions. However, deep learning diverges from machine learning in many different aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional Machine Learning\n",
    "- Typically does not require a GPU\n",
    "- Typically relies on hand-crafted feature engineering\n",
    "- Requires relatively less data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning\n",
    "- Models have a lot of parameters\n",
    "- Often requires a GPU\n",
    "- Can learn features from large, unstructured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "lst = [[1,2,3],[4,5,6]]\n",
    "tensor = torch.tensor(lst)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "array = [1,2,3,4]\n",
    "np_array = np.array(array)\n",
    "np_tensor = torch.tensor(np_array)\n",
    "print(np_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.int64\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "lst = [[1,2,3],[4,5,6]]\n",
    "tensor = torch.tensor(lst)\n",
    "print(tensor.shape)\n",
    "print(tensor.dtype)\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 3],\n",
      "        [5, 5]])\n",
      "tensor([[3, 3],\n",
      "        [5, 5]])\n",
      "tensor([[-1, -1],\n",
      "        [-1, -1]])\n",
      "tensor([[-1, -1],\n",
      "        [-1, -1]])\n",
      "tensor([[2, 2],\n",
      "        [6, 6]])\n",
      "tensor([[2, 2],\n",
      "        [6, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Compatible shapes\n",
    "a = torch.tensor([[1,1],\n",
    "                  [2,2]])\n",
    "b = torch.tensor([[2,2],\n",
    "                 [3,3]])\n",
    "print(a+b)\n",
    "print(torch.add(a,b))\n",
    "\n",
    "print(a-b)\n",
    "print(torch.sub(a,b))\n",
    "\n",
    "print(a*b)\n",
    "print(torch.mul(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      3\u001b[0m                   [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m]])\n\u001b[0;32m      4\u001b[0m c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m],\n\u001b[0;32m      5\u001b[0m                   [\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m5\u001b[39m]])\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43ma\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mc\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Incompatible shapes which gives error\n",
    "a = torch.tensor([[1,1],\n",
    "                  [2,2]])\n",
    "c = torch.tensor([[2,2,4],\n",
    "                  [3,3,5]])\n",
    "print(a+c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating tensors and accessing attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are the primary data structure in PyTorch and will be the building blocks for our deep learning models. They share many similarities with NumPy arrays but have some unique attributes too.\n",
    "\n",
    "In this exercise, you'll practice creating a tensor from a Python list and displaying some of its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "\n",
    "list_a = [1, 2, 3, 4]\n",
    "\n",
    "# Create a tensor from list_a\n",
    "tensor_a = torch.tensor(list_a)\n",
    "\n",
    "print(tensor_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "\n",
    "list_a = [1, 2, 3, 4]\n",
    "\n",
    "# Create a tensor from list_a\n",
    "tensor_a = torch.tensor(list_a)\n",
    "\n",
    "# Display the tensor device\n",
    "print(tensor_a.device)\n",
    "\n",
    "# Display the tensor data type\n",
    "print(tensor_a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating tensors from NumPy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are the fundamental data structure of PyTorch. You can create complex deep learning algorithms by learning how to manipulate them.\n",
    "\n",
    "The torch package has been imported, and two NumPy arrays have been created, named array_a and array_b. Both arrays have the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "array_a = np.array([[1,1,1],\n",
    "                    [2,3,4],\n",
    "                    [4,5,6]])\n",
    "array_b = np.array([[7,5,4],\n",
    "                   [2,2,8],\n",
    "                   [6,3,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  1,  1],\n",
      "        [ 4,  7, 28],\n",
      "        [22, 17, 46]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Create two tensors from the arrays\n",
    "tensor_a = torch.tensor(array_a)\n",
    "tensor_b = torch.tensor(array_b)\n",
    "\n",
    "# Subtract tensor_b from tensor_a \n",
    "tensor_c = tensor_a - tensor_b\n",
    "\n",
    "# Multiply each element of tensor_a with each element of tensor_b\n",
    "tensor_d = tensor_a * tensor_b\n",
    "\n",
    "# Add tensor_c to tensor_d\n",
    "tensor_e = tensor_c + tensor_d\n",
    "print(tensor_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2839, 0.2779]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create input_tensor with three features\n",
    "input_tensor = torch.tensor(\n",
    "    [[0.3471, 0.4547, -0.2356]]\n",
    ")\n",
    "\n",
    "# A linear layer takes an input, applies a linear function and returns the output\n",
    "\n",
    "# Define our first linear layer\n",
    "linear_layer = nn.Linear(in_features = 3, out_features = 2)\n",
    "\n",
    "# Pass input through linear layer\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3839, -0.1537, -0.1121],\n",
       "        [ 0.0429,  0.2808, -0.0013]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.4607, 0.1351], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3472, -0.0120,  0.0420, -0.0964,  0.1605]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create network with three linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10,18),\n",
    "    nn.Linear(18,20),\n",
    "    nn.Linear(20,5)\n",
    ")\n",
    "\n",
    "input_tensor = torch.tensor([[-0.0014, 0.4038, 1.0305, 0.7521, 0.2942, -0.8694, 0.3132, 0.1351, -0.7585, 0.2893]])\n",
    "\n",
    "# Pass input_tensor through model to get output\n",
    "output_tensor = model(input_tensor)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your first neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a small neural network containing two linear layers. The first layer takes an eight-dimensional input, and the last layer outputs a one-dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2069]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "# Implement a small neural network with exactly two linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 5),\n",
    "    nn.Linear(5, 1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9975]])\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid Function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.tensor([[6.0]])\n",
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function as last layer\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6,4), # First Linear Layer\n",
    "    nn.Linear(4,1), # Second Linear Layer\n",
    "    nn.Sigmoid() # Sigmoid Activation Function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1392, 0.8420, 0.0188]])\n"
     ]
    }
   ],
   "source": [
    "## Getting familiar with softmax\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create an input tensor\n",
    "input_tensor = torch.tensor(\n",
    "    [[4.3, 6.1, 2.3]]\n",
    ")\n",
    "\n",
    "# Apply softmax along the last dimension\n",
    "probabilities = nn.Softmax(dim = -1) # dim = -1 indicates softmax is applied to the input tensor's last dimension \n",
    "output_tensor = probabilities(input_tensor)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The sigmoid and softmax functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid and softmax functions are two of the most popular activation functions in deep learning. They are both usually used as the last step of a neural network. Sigmoid functions are used for binary classification problems, whereas softmax functions are often used for multi-class classification problems. This exercise will familiarize you with creating and using both functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6900]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[0.8]])\n",
    "\n",
    "# Create a sigmoid function and apply it on input_tensor\n",
    "sigmoid = nn.Sigmoid()\n",
    "probability = sigmoid(input_tensor)\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2828e-01, 1.1698e-04, 5.7492e-01, 3.4961e-02, 1.5669e-01, 1.0503e-01]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n",
    "\n",
    "# Create a softmax function and apply it on input_tensor\n",
    "softmax = nn.Softmax(dim = -1)\n",
    "probabilities = softmax(input_tensor)\n",
    "print(probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
