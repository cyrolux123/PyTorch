{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Classification: forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input data of shape 5*6\n",
    "input_data = torch.tensor(\n",
    "    [[-0.4421, 1.5207, 2.0607, -0.3647, 0.4691, 0.0946],\n",
    "     [0.4838, 0.4001, 0.9786, 0.7858, 0.9876, 0.5987],\n",
    "     [0.1902, 0.8507, 0.8175, 0.0994, 0.3862, 0.0132],\n",
    "     [0.1973, 0.6625, 0.8123, 0.4483, 0.0345, 0.4165],\n",
    "     [0.6948, 0.3557, 0.7689, 0.2793, 0.6816, 0.9152]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary classification model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6,4), # First linear layer\n",
    "    nn.Linear(4,1), # Second linear layer\n",
    "    nn.Sigmoid() # Sigmoid activation function\n",
    ")\n",
    "\n",
    "# Pass input data through model\n",
    "output = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4647],\n",
      "        [0.4493],\n",
      "        [0.4568],\n",
      "        [0.4505],\n",
      "        [0.4582]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output) \n",
    "# Ouputs are five probabilities between 0 and 1\n",
    "# It gives one value for each sample(row) in data\n",
    "\n",
    "# Classification\n",
    "# Class = 0 for first and third values\n",
    "# Class = 1 for second, fourth and fifth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# Specify model has three classes\n",
    "n_classes = 3\n",
    "\n",
    "# Create multi-class classification model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6,4), # First linear layer\n",
    "    nn.Linear(4,n_classes), # Second linear layer\n",
    "    nn.Softmax(dim = -1) # Softmax activation function\n",
    ")\n",
    "\n",
    "# Pass input data through model\n",
    "output = model(input_data)\n",
    "print(output.shape) # Output shape is 5*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2884, 0.4384, 0.2732],\n",
      "        [0.3075, 0.4194, 0.2731],\n",
      "        [0.3294, 0.4034, 0.2672],\n",
      "        [0.3096, 0.4249, 0.2655],\n",
      "        [0.2933, 0.4384, 0.2683]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "\n",
    "# Each row sums to one\n",
    "# Value with highest probability is assigned predicted label in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1507],\n",
      "        [0.7421],\n",
      "        [0.3649],\n",
      "        [0.3432],\n",
      "        [0.8036]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Regression : Forwards pass\n",
    "\n",
    "# Create regression model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6,4), # First linear layer\n",
    "    nn.Linear(4,1) # Second linear layer\n",
    ")\n",
    "\n",
    "# Pass input data through model\n",
    "output = model(input_data)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a binary classifier in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a small neural network with a single linear layer followed by a sigmoid function is a binary classifier. It acts just like a logistic regression.\n",
    "\n",
    "In this exercise, you'll practice building this small network and interpreting the output of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0518]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a small neural network for binary classification\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(8,1),\n",
    "  nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2995]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Regression model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a neural network with exactly four linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(11, 8),\n",
    "    nn.Linear(8, 4),\n",
    "    nn.Linear(4, 2),\n",
    "    nn.Linear(2, 1)\n",
    "    )\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0819, 0.0649, 0.4576, 0.3957]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Update network below to perform a multi-class classification with four labels\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4), \n",
    "  nn.Softmax(dim = -1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE Manually\n",
    "import numpy as np\n",
    "one_hot_numpy = np.array([1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prevent doing manually\n",
    "\n",
    "import torch.nn.functional as f\n",
    "f.one_hot(torch.tensor(0), num_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.one_hot(torch.tensor(1), num_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.one_hot(torch.tensor(2), num_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8131, dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross Entropy Loss in PyTorch\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "scores = torch.tensor([[-0.1211, 0.1059]])\n",
    "one_hot_target = torch.tensor([[1,0]])\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "criterion(scores.double(), one_hot_target.double())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating one-hot encoded labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is a technique that turns a single integer label into a vector of N elements, where N is the number of classes in your dataset. This vector only contains zeros and ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([1 if i == y else 0 for i in range(num_classes)])\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = torch.nn.functional.one_hot(torch.tensor(y), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy loss is the most used loss for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
    "\n",
    "# Create the cross entropy loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0670,  0.2443,  0.1667,  0.0924,  0.1875, -0.0193,  0.0370, -0.1299,\n",
       "          -0.0245, -0.0327,  0.1486, -0.1074, -0.0208, -0.0623,  0.0813, -0.1135],\n",
       "         [ 0.0936, -0.3415, -0.2330, -0.1292, -0.2622,  0.0269, -0.0518,  0.1816,\n",
       "           0.0342,  0.0457, -0.2077,  0.1501,  0.0291,  0.0871, -0.1136,  0.1587],\n",
       "         [ 0.0276, -0.1005, -0.0686, -0.0380, -0.0771,  0.0079, -0.0152,  0.0535,\n",
       "           0.0101,  0.0134, -0.0611,  0.0442,  0.0086,  0.0256, -0.0334,  0.0467],\n",
       "         [-0.0018,  0.0065,  0.0044,  0.0024,  0.0050, -0.0005,  0.0010, -0.0034,\n",
       "          -0.0006, -0.0009,  0.0039, -0.0028, -0.0006, -0.0016,  0.0021, -0.0030],\n",
       "         [ 0.0908, -0.3312, -0.2260, -0.1253, -0.2543,  0.0261, -0.0502,  0.1762,\n",
       "           0.0332,  0.0443, -0.2014,  0.1456,  0.0282,  0.0844, -0.1102,  0.1539],\n",
       "         [-0.0066,  0.0242,  0.0165,  0.0091,  0.0185, -0.0019,  0.0037, -0.0129,\n",
       "          -0.0024, -0.0032,  0.0147, -0.0106, -0.0021, -0.0062,  0.0080, -0.0112],\n",
       "         [-0.0172,  0.0626,  0.0427,  0.0237,  0.0481, -0.0049,  0.0095, -0.0333,\n",
       "          -0.0063, -0.0084,  0.0381, -0.0275, -0.0053, -0.0160,  0.0208, -0.0291],\n",
       "         [ 0.0291, -0.1061, -0.0724, -0.0401, -0.0814,  0.0084, -0.0161,  0.0564,\n",
       "           0.0106,  0.0142, -0.0645,  0.0466,  0.0090,  0.0270, -0.0353,  0.0493]]),\n",
       " tensor([-0.1641,  0.2294,  0.0675, -0.0043,  0.2224, -0.0162, -0.0421,  0.0712]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model and run a forward pass\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(16,8),\n",
    "    nn.Linear(8,4),\n",
    "    nn.Linear(4,2)\n",
    ")\n",
    "\n",
    "# Sample input tensor\n",
    "sample = torch.randn(1, 16)\n",
    "\n",
    "# Sample target tensor (class index)\n",
    "target = torch.tensor([1])\n",
    "\n",
    "prediction = model(sample)\n",
    "\n",
    "# Calculate the loss and compute the gradients\n",
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion(prediction, target)\n",
    "loss.backward()\n",
    "\n",
    "# Access each layer's gradients\n",
    "model[0].weight.grad, model[0].bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3443, -0.5129, -0.1063,  0.0071,  0.1235,  0.1755, -0.0025, -0.0056],\n",
       "         [-0.2651,  0.3949,  0.0819, -0.0055, -0.0951, -0.1351,  0.0020,  0.0043],\n",
       "         [-0.3465,  0.5162,  0.1070, -0.0072, -0.1243, -0.1766,  0.0026,  0.0056],\n",
       "         [ 0.3472, -0.5172, -0.1072,  0.0072,  0.1246,  0.1770, -0.0026, -0.0056]]),\n",
       " tensor([ 0.3906, -0.3007, -0.3931,  0.3939]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[1].weight.grad, model[1].bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2291, -0.1491,  0.0316, -0.2477],\n",
       "         [ 0.2291,  0.1491, -0.0316,  0.2477]]),\n",
       " tensor([ 0.4556, -0.4556]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[2].weight.grad, model[2].bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating model parameters manually\n",
    "\n",
    "# learning rate is typically small\n",
    "lr = 0.001\n",
    "\n",
    "# Update the weights\n",
    "weight = model[0].weight\n",
    "weight_grad = model[0].weight.grad\n",
    "weight = weight - lr * weight_grad\n",
    "\n",
    "# Update the biases\n",
    "bias = model[0].bias\n",
    "bias_grad = model[0].bias.grad\n",
    "bias = bias - lr * bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(8, 2))\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[2].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
